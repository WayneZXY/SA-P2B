{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging # 引入logging模块\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "# 第一步，创建一个logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Log等级总开关\n",
    "# 第二步，创建一个handler，用于写入日志文件\n",
    "rq = time.strftime('%Y%m%d%H%M', time.localtime(time.time()))\n",
    "# log_path = os.path.dirname(os.getcwd()) + '/Logs/'\n",
    "log_path = os.getcwd() + '/Logs/'\n",
    "# print(log_path)\n",
    "log_name = log_path + rq + '.log'\n",
    "logfile = log_name\n",
    "# os.makedirs(log_path)\n",
    "fh = logging.FileHandler(logfile)\n",
    "fh.setLevel(logging.DEBUG) # 输出到file的log等级的开关\n",
    "# 第三步，定义handler的输出格式\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "# 第四步，将logger添加到handler里面\n",
    "logger.addHandler(fh)\n",
    "# 日志\n",
    "logger.debug('this is a logger debug message')\n",
    "logger.info('this is a logger info message')\n",
    "logger.warning('this is a logger warning message')\n",
    "logger.error('this is a logger error message')\n",
    "logger.critical('this is a logger critical message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "haha\n2\nyoyo\nlala\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class A(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(A, self).__init__()\n",
    "        self.c = c\n",
    "        print('haha')\n",
    "        print(self.c)\n",
    "    \n",
    "    def forward(self,):\n",
    "        print('lala')\n",
    "        return self.c + 1\n",
    "\n",
    "class B(A):\n",
    "\n",
    "    def __init__(self, e):\n",
    "        super(B, self).__init__(c=e)\n",
    "        self.c += 1\n",
    "        print('yoyo')\n",
    "    \n",
    "    # def forward(self):\n",
    "    #     print('yoyo2')\n",
    "\n",
    "class C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C, self).__init__()\n",
    "        self.backbone = A()\n",
    "        print('sasa')\n",
    "\n",
    "    def forward(self):\n",
    "        print('gaga')\n",
    "        return self.backbone(1)\n",
    "\n",
    "b = B(2)\n",
    "b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([3, 256, 126, 125]), torch.Size([3, 256, 126, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "a = torch.ones((3, 256, 126, 125))\n",
    "b = F.max_pool2d(a, kernel_size=[1, a.size(3)])\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import etw_pytorch_utils as pt_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pointnet2.utils.pointnet2_modules import PointnetSAModule\n",
    "# A = (\n",
    "#                 pt_utils.Seq(256)\n",
    "#                 .conv1d(1, bn=True, activation=None))\n",
    "A = nn.Linear(256, 3)\n",
    "a = torch.ones((3, 256, 1024))\n",
    "# b = A(a).squeeze(1)\n",
    "# b = b.sigmoid()\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "yada: 100%|#########################################################| 50/50 [00:05<00:00,  9.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "for i in tqdm(range(50), ascii=True, ncols=100, position=0, desc='yada'):\n",
    "    # tqdm.write('hoho')\n",
    "    time.sleep(0.1)\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "all annotations: 100%|██████████| 19522/19522 [03:10<00:00, 102.42it/s]\n",
      "annotations of a certain instance: 100%|██████████| 441/441 [02:06<00:00,  3.50it/s]\n",
      "yada: 100%|##########| 9761/9761 [07:34<00:00, 21.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from Dataset import SiameseTrain\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "train_data = SiameseTrain(\n",
    "            input_size=1024,\n",
    "            path='/media/zhouxiaoyu/本地磁盘/RUNNING/P2B/data/traning',\n",
    "            split='Train',\n",
    "            category_name='Car',\n",
    "            offset_BB=0,\n",
    "            scale_BB=1.25)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True)\n",
    "for i, data in enumerate(tqdm(train_dataloader, ascii=True, position=0, desc='yada')):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# for idx, d in enumerate(data):\n",
    "    # print(data[idx].shape)\n",
    "# from train_tracking import aux_loss\n",
    "import torch \n",
    "from mmdet.core.loss.losses import weighted_smoothl1, weighted_sigmoid_focal_loss\n",
    "label_point_set, label_cla, label_reg, object_point_set, sample_seg_label, sample_seg_offset = data\n",
    "rpn_cls_target = sample_seg_label.float()\n",
    "pos = (sample_seg_label > 0).float()\n",
    "neg = (sample_seg_label == 0).float()\n",
    "# pos.shape, neg.shape\n",
    "pos_normalizer = pos.sum()\n",
    "\n",
    "pos_normalizer = torch.clamp(pos_normalizer, min=1.0)\n",
    "# pos_normalizer\n",
    "cls_weights = pos + neg\n",
    "cls_weights = cls_weights / pos_normalizer\n",
    "# cls_weights\n",
    "reg_weights = pos\n",
    "reg_weights = reg_weights / pos_normalizer\n",
    "# rpn_cls_target.view(-1).shape\n",
    "point_cls = torch.randn((8, 1024))\n",
    "point_reg = torch.randn((8, 3, 1024))\n",
    "# aux_loss_cls = weighted_sigmoid_focal_loss(point_cls, rpn_cls_target, weight=cls_weights, avg_factor=1.)\n",
    "# aux_loss_cls\n",
    "\n",
    "# aux_loss_reg = weighted_smoothl1(point_reg.transpose(1, 2), sample_seg_offset, beta=1 / 9., weight=reg_weights[..., None], avg_factor=1.)\n",
    "# aux_loss_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 2, 2]),\n",
       " torch.Size([1, 2, 3, 2, 2, 1, 1]),\n",
       " torch.Size([2, 3, 2, 2, 1, 1, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import torch as t \n",
    "a = t.randn(2, 3, 2, 2)\n",
    "# mean = a.mean([3])\n",
    "b = a[None, ..., None, None]\n",
    "c = a[..., None, None, None]\n",
    "a.shape, b.shape, c.shape\n",
    "# a, mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([8, 256, 128]), torch.Size([8, 128, 3]))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from pointnet2.models.pointnet_tracking import Pointnet_Backbone\n",
    "import torch \n",
    "backbone_net = Pointnet_Backbone(0, True).cuda()\n",
    "template = torch.randn((8, 1024, 3)).cuda()\n",
    "template_xyz, template_feature = backbone_net(template, [512, 256, 128])\n",
    "template_feature.shape, template_xyz.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[-1.20219052, -0.21663953, -0.56849238],\n",
       "        [-1.24969502, -0.03615362,  0.85302042],\n",
       "        [-0.00914169, -1.0164954 ,  0.99769676],\n",
       "        [ 0.09099028, -1.36884163, -1.41894603],\n",
       "        [ 1.7147394 ,  1.15809206,  0.23330085],\n",
       "        [ 0.32702668,  0.65608341,  0.80417648],\n",
       "        [ 0.79787696, -0.00958418,  1.37103841],\n",
       "        [ 1.6886309 , -1.9884007 ,  0.98380109],\n",
       "        [ 1.27157148, -1.83970546, -1.17168784],\n",
       "        [-1.25529617,  1.16808271, -0.17124095]]),\n",
       " array([[-2.20219052, -2.21663953, -3.56849238],\n",
       "        [-2.24969502, -2.03615362, -2.14697958],\n",
       "        [-1.00914169, -3.0164954 , -2.00230324],\n",
       "        [-0.90900972, -3.36884163, -4.41894603],\n",
       "        [ 0.7147394 , -0.84190794, -2.76669915],\n",
       "        [-0.67297332, -1.34391659, -2.19582352],\n",
       "        [-0.20212304, -2.00958418, -1.62896159],\n",
       "        [ 0.6886309 , -3.9884007 , -2.01619891],\n",
       "        [ 0.27157148, -3.83970546, -4.17168784],\n",
       "        [-2.25529617, -0.83191729, -3.17124095]]))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import numpy as np\n",
    "label_reg = [1, 2, 3, 4]\n",
    "reg = np.array(label_reg)\n",
    "points = np.random.randn(10, 3)\n",
    "offset = points - reg[:3]\n",
    "points, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn((2, 3, 4))\n",
    "b = a.transpose(0, 1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('zhou': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8260925e8311b679e92b1ebddef16cbd7b6fd9d1cab3ac4acda3a1d3485d79f7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}